# 视觉与AI历史

2018年11月15日

## 史前文明

在上个世纪40、50年代，就有一大批科学家开始探讨人工大脑的可能性。最初的人工智能研究是30年代末到50年代初一系列科学交汇到产物。神经学研究发现大脑是由神经元组成的电子网络，其激励电平只有“有”或“无”两种状态。维纳的控制论描述了电子网络的控制和稳定性。克劳德·香农提出的信息论则描述了数字信号（即高低电平代表的二进制信号）。图灵的计算理论证明数字信号足以描述任何形式的计算。这些密切相关的想法暗示了构建电子大脑的可能性。

Walter Pitts和Warren McCulloch分析了理想化的人工神经元网络，并且指出了它们进行简单逻辑运算的机制。他们是最早描述所谓“神经网络”的学者。马文·明斯基是他们的学生，当时是一名24岁的研究生。1951年他与Dean Edmonds一道建造了第一台神经网络机，称为SNARC。在接下来的五十年中，明斯基是AI领域最重要的领导者和创新者之一。

1951年，Christopher Strachey使用曼彻斯特大学的Ferranti Mark 1机器写出了一个西洋跳棋（checkers）程序；Dietrich Prinz则写出了一个国际象棋程序。亞瑟·山谬尔（Arthur Samuel）在五十年代中期和六十年代初开发的西洋棋程序的棋力已经可以挑战具有相当水平的业余爱好者。游戏AI一直被认为是评价AI进展的一种标准。

1950年，图灵发表了一篇划时代的论文*Computing Machinery and Intelligence*，文中预言了创造出具有真正智能的机器的可能性。由于注意到“智能”这一概念难以确切定义，他提出了著名的**图灵测试**：如果一台机器能够与人类展开对话（通过电传设备）而不能被辨别出其机器身份，那么称这台机器具有智能。**图灵测试是人工智能哲学方面第一个严肃的提案**，它让机器学习成本有了可落地的方案。

50年代中期，随着数字计算机的兴起，一些**科学家直觉地感到可以进行数字操作的机器也应当可以进行符号操作，而符号操作可能是人类思维的本质**。这是创造智能机器的一条新路。

1955年，艾伦·纽厄尔和后来荣获诺贝尔奖的赫伯特·西蒙在J. C. Shaw的协助下开发了“逻辑理论家（Logic Theorist）”。这个程序能够证明《数学原理》中前52个定理中的38个，其中某些证明比原著更加新颖和精巧。Simon认为他们已经“解决了神秘的心/身问题，解释了物质构成的系统如何获得心灵的性质。” （这一断言的哲学立场后来被John Searle称为“强人工智能”，即机器可以像人一样具有思想。）

## 人工智能时代的到来

1956年达特茅斯会议标志着“人工智能”的诞生，人工智能从此被确立为一门学科。会议组织者约翰·麦卡锡 (John McCarthy) 首次提出了“人工智能”这个概念，AI的名称和使命得以确定。这个会议提出断言：“学习和智能的任何特性都应被精准描述，以让机器模仿。” ("every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it")。

1958年，Frank Rosenblatt提出感知器是神经网络的一种形式，并预言：“感知器最终将能够学习，作出决策和翻译语言”。

1959年，Hubel 和  Wiesel 研究瞳孔区域与大脑皮层神经元的对应关系。他们在一只麻醉猫后脑头骨开了个3毫米的洞，将微电极插入猫的初级视觉皮层 (primary visual cortex) ，测量神经元的活跃程度。经过很多天枯燥的试验，同时牺牲了若干只小猫，他们终于发现了“**方向选择性细胞（Orientation Selective Cell）**”。当瞳孔发现物体边缘（Edge）指向某个方向时，这种神经元细胞就会活跃。就此，他们提出了**感受区（Receptive Field）**的概念。

![Hubel & Wiesel, 1959](https://github.com/AarioAi/share/blob/master/_asset/1-1959-hubel-wiesel.png?raw=true)

* 1965年，Larry Roberts 博士论文提出机器视觉先导性工作"Block world"，通过计算机提取图片中物体的边缘。
 ![Larry Roberts, Block world](https://github.com/AarioAi/share/blob/master/_asset/1-larry-roberts-blockworld.jpg?raw=true)

1966年，麻省理工学院发表第一篇关于计算机视觉的论文。

1970年，David Marr 发表著作《Vision: A computaional investigaion  into the human representation and processing of visual information》。

许多AI程序使用相同的基本算法。为实现一个目标（例如赢得游戏或证明定理），它们一步步地前进，就像在迷宫中寻找出路一般；如果遇到了死胡同则进行回溯。这就是“**搜索式推理**”。

这一思想遇到的主要困难是，在很多问题中，“迷宫”里可能的线路总数是一个天文数字（所谓“指数爆炸”）。研究者使用启发式算法去掉那些不太可能导出正确答案的支路，从而缩小搜索范围。

达特茅斯会议之后的数年是大发现的时代。对许多人而言，这一阶段开发出的程序堪称神奇：计算机可以解决代数应用题，证明几何定理，学习和使用英语。当时大多数人几乎无法相信机器能够如此“智能”。研究者们在私下的交流和公开发表的论文中表达出相当乐观的情绪，认为具有完全智能的机器将在二十年内出现。

在接下来到10几年内，第一代AI研究者们曾作出了很多过于乐观的预言：

* 1958年，艾倫·紐厄爾和赫伯特·西蒙：“十年之内，数字计算机将成为国际象棋世界冠军。” “十年之内，数字计算机将发现并证明一个重要的数学定理。”
* 1965年，赫伯特·西蒙：“二十年内，机器将能完成人能做到的一切工作。”
* 1968年，科幻作品《“2001太空漫游”》一书中设想2001年将会出现达到或超过人类智能的机器。
* 1967年，馬文·閔斯基：“一代之内……创造‘人工智能’的问题将获得实质上的解决。”
* 1970年，馬文·閔斯基：“在三到八年的时间里我们将得到一台具有人类平均智能的机器。”

受限于当时计算机运算能力不足，到了70年代，AI开始遭遇批评，随之而来的还有资金上的困难。AI研究者们对其课题的难度未能作出正确判断：此前的过于乐观使人们期望过高，当承诺无法兑现时，对AI的资助就缩减或取消了。同时，由于马文·闵斯基对感知器的激烈批评，联结主义（即神经网络）销声匿迹了十年。

1974年，AI研究经费削减，第一次“AI之冬”来临。

到了80年代，一类名为“**专家系统**”的程序证明了AI的实用性。

1981年，日本拨款8亿5千万支持第五代计算机项目，用于AI相关项目的研究。英国拨款3亿5千万英镑用于Alvey工程……

1982年，物理学家John Hopfield证明一种新型的神经网络（现被称为“Hopfield网络”）能够用一种全新的方式学习和处理信息。大约在同时（早于Paul Werbos），David Rumelhart推广了反向传播算法，一种神经网络训练方法。这些发现使1970年以来一直遭人遗弃的联结主义重获新生。

1987年，David Lower 提出边缘检测方法。

AI再一次获得了成功。政府和商业投资的疯狂追捧，导致了80年代末90年代初AI泡沫的破灭。

随着计算机性能的大幅度提升，AI 开始了里程碑的时代。

1997年5月11日，深蓝成为战胜国际象棋世界冠军卡斯帕罗夫的第一个计算机系统。

1999年， David Lowe发表SIFT（Scale-invariant feature transform：尺度不变特征变换）算法。这种描述具有尺度不变性，可在图像中检测出关键点，是一种局部特征描述子。

2005年，Stanford开发的一台机器人在一条沙漠小径上成功地自动行驶了131英里，赢得了DARPA挑战大赛头奖。
2006年，Lazebnik、Schmid和Ponce提出了SPM（Spatial Pyramid matching:空间金字塔匹配）。

2009年，Felzenswalb提出DPM（Deformable Parts Model）算法。目前已成为众多分类器、分割、人体姿态和行为分类的重要部分。DPM可以看做是HOG（Histogrrams of Oriented Gradients）的扩展，大体思路与HOG一致。先计算梯度方向直方图，然后用SVM（Surpport Vector Machine ）训练得到物体的梯度模型（Model）。有了这样的模板就可以直接用来分类了，简单理解就是模型和目标匹配。DPM只是在模型上做了很多改进工作。

2009年，藍腦計畫声称已经成功地模拟了部分鼠脑。

90年代的许多AI研究者故意用其他一些名字称呼他们的工作，例如信息学，知识系统，认知系统或计算智能。部分原因是他们认为他们的领域与AI存在根本的不同，不过新名字也有利于获取经费。至少在商业领域，导致AI之冬的那些未能兑现的承诺仍然困扰着AI研究，正如New York Times在2005年的一篇报道所说：“计算机科学家和软件工程师们避免使用人工智能一词，因为怕被认为是在说梦话。”

## 深度学习

21世纪，得益于大数据和计算机算力的快速发展，AI 进入了爆发期。麦肯锡全球研究院在一份题为《大数据：创新、竞争和生产力的下一个前沿领域》的报告中估计，到2009年，美国经济所有行业中具有1000名以上员工的公司都至少平均拥有一个200兆兆字节的存储数据。大数据应用也开始逐渐渗透到其他领域，例如生态学模型训练、经济领域中的各种应用、医学研究中的疾病预测及新药研发等。深度学习（特别是深度卷积神经网络和循环网络）更是极大地推动了图像和视频处理、文本分析、语音识别等问题的研究进程。

深度学习是机器学习的一个分支，它通过一个有着很多层处理单元的深层网络对数据中的高级抽象进行建模。根据全局逼近原理（Universal approximation theorem），对于神经网络而言，如果要拟合任意连续函数，深度性并不是必须的，即使一个单层的网络，只要拥有足够多的非线性激活单元，也可以达到拟合目的。但是，目前深度神经网络得到了更多的关注，这主要是源于其结构层次性，能够快速建模更加复杂的情况，同时避免浅层网络可能遭遇的诸多缺点。

然而，深度学习也有自身的缺点。以循环神经网络为例，一个最常见的问题是梯度消失问题（沿着时间序列反向传播过程中，梯度逐渐减小到0附近，造成学习停滞）。为了解决这些问题，很多针对性的模型被提出来，例如LSTM（长短期记忆网络，早在1997年就已经提出，最近随着RNN的大火，又重新进入大众视野）、GRU（门控循环神经单元）等等。

现在，最先进的神经网络结构在某些领域已经能够达到甚至超过人类平均准确率，例如在计算机视觉领域，特别是一些具体的任务上，比如MNIST数据集（一个手写数字识别数据集）、交通信号灯识别等。再如游戏领域，Google的deepmind团队研发的AlaphaGo，在问题搜索复杂度极高的围棋上，已经打遍天下无敌手。

2016年6月，吴恩达跟谷歌科学家合作，用1.6万台计算机搭建并模拟了一个人脑神经网络，向这个网络展示了1000万段YouTube视频。这个系统自己认识到“猫”是一种怎样的动物，并成功找到了猫的照片，识别率为81.7%。“识别猫”成为深度学习领域的经典案例。

## 相关资料

* 《Receptive fields of single neurones in the cat's striate cortext》 Hubel & Wiesel
  * https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf