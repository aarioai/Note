# Mem0 长记忆

[Mem0](https://github.com/mem0ai/mem0)

Mem0 (pronounced as "mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. Mem0 remembers user preferences, adapts to individual needs, and continuously improves over time, making it ideal for customer support chatbots, AI assistants, and autonomous systems.


**Memory Add:**
1. 将一个或多个对话通过一个LLM（如 deepseek_r1）抽取为相关记忆点（即简短、概括的语句），存入向量数据库
2. Add/Delete/Update 时候，也需要通过该LLM来抽取相关记忆点，再操作进向量数据库

```                                                           
                                  memories             <------
                                    -----> [Vector DB] ------> 
conversation ---> LLM (extraction)                            LLM (update)
                                    -----> [Graph DB]  ------>
                            entities & relations       <------
```

Vector Database Index: 将向量化数据库通过HNSW等方式建立索引

**Memory Search:**
1. 将对话通过一个Embedding LLM（可不同于extraction LLM）进行向量化
2. 将向量作为查询，通过向量索引查询

```
            rewritten query            vector search
                ----> <Embedding Model> --------> [Vector DB] ---->
query ---> LLM                                                        result
                ----> <Graph> ---graph search---> [Graph DB]  ----> 
```

## SDK Basic Usage
Mem0 requires an LLM to function, with gpt-4o-mini from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/overview).

```python
from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = "default_user") -> str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_memories["results"])
    
    # Generate Assistant response
    system_prompt = f"You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}"
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": message}]
    response = openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({"role": "assistant", "content": assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print("Chat with AI (type 'exit' to quit)")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == 'exit':
            print("Goodbye!")
            break
        print(f"AI: {chat_with_memories(user_input)}")

if __name__ == "__main__":
    main()
```

### SDK Methods
[Python SDK](https://docs.mem0.ai/open-source/python-quickstart)

```python
memory.add("Likes reading on weekends", user_id="Aario", metadata={"category":"hobbies"})
memory.update(memory_id="m1", data="Likes to play cricket on weekends")
# memory.delete(memory_id="m1")
# memory.delete_all(user_id="Aario")
# memory.reset() 
```
```json lines
// history = memory.history(memory_id="m1")
[
  {
    'id': 'h1',
    'memory_id': 'm1',
    'prev_value': None,
    'new_value': 'Likes reading on weekends',
    'event': 'add',
    'timestamp': '2025-04-14 10:00:54.466687',
    'is_deleted': 0
  },
  {
    'id': 'h2',
    'memory_id': 'm1',
    'prev_value': 'Likes to play cricket on weekends',
    'new_value': 'Likes to play tennis on weekends',
    'event': 'update',
    'timestamp': '2025-04-14 10:15:17.230943',
    'is_deleted': 0
  }
]
```


```json lines
// results = memory.get_all()
[
  {
    'id': 'm1',
    'event': 'add',
    'metadata':{
        'data': 'Likes to play cricket on weekends',
        'category': 'hobbies',
    }
  }
]
```

```json lines
// results = memory.get("m1)
{
    'id': 'm1',
    'event': 'add',
    'metadata':{
        'data': 'Likes to play cricket on weekends',
        'category': 'hobbies',
    }
}
```

```json lines
// results = memory.search(query="What is Aario's hobbies?", user_id="Aario")
[
  {
    'id': 'm1',
    'text': 'Likes to play cricket on weekends',
    'metadata': {
      'data': 'Likes to play cricket on weekends',
      'category': 'hobbies'
    },
    'score': 0.85 
  },
  // other results
]
```

## SDK Use OSS Config

### Vector Database Config
[Vector Database Config](https://docs.mem0.ai/components/vectordbs/overview)

**Qdrant:**
```python
mem0_config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name":"demo",
            "embedding_model_dims": "1536",
            "client": None,
            "host": "localhost",
            "port":6333,
            "path":"/tmp/qdrant",
            "url":None,
            "api_key":None,
            "on_disk":False,
        }
    },
}

memc = Memory.from_config(mem0_config)
```

**Chroma:**
```python
mem0_config = {
    "vector_store": {
        "provider": "chroma",
        "config": {
            "collection_name":"demo",
            "client": None,
            "path": "db",
            "host": None,
            "port": None,
        }
    },
}

memc = Memory.from_config(mem0_config)
```

**Milvus:**
```python
mem0_config = {
    "vector_store": {
        "provider": "milvus",
        "config": {
            "url": "http://localhost:19530",
            "token": None,
            "collection_name":"demo",
            "embedding_model_dims": "1536",
            "metric_type": "L2",
        }
    },
}

memc = Memory.from_config(mem0_config)
```

**Faiss:**
```python
mem0_config = {
    "vector_store": {
        "provider": "faiss",
        "config": {
            "collection_name":"demo",
            "path": "/tmp/faiss/<collection_name>",
            "distance_strategy": "euclidean",
            "normalize_L2": False,
        }
    },
}

memc = Memory.from_config(mem0_config)
```

**LangChain:**
```python
import os
from mem0 import Memory
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# Initialize a LangChain vector store
embeddings = OpenAIEmbeddings()
vector_store = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
    collection_name="demo"  
)

mem0_config = {
    "vector_store": {
        "provider": "langchain",
        "config": {
            "client": vector_store
        }
    }
}

memc = Memory.from_config(mem0_config)
```

### Huoshan Longmem Usage
```python
from mem0 import Memory

mem0_config = {
    "vector_store": {
        "provider": "vikingdb",
        "config": {
            "collection_name": "demo",
        }
    },
    "llm": {
        "provider": "doubao",
        "config": {
            "model": SUMMARY_ENDPOINT,
        }
    },
    "embedder": {
        "provider": "doubao",
        "config": {
            "model": EMBEDDING_ENDPOINT,
        }
    },
    "custom_prompt": SUMMARY_PROMPT,
}
memory_client = Memory.from_config(mem0_config)
```