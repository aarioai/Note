# AI 硬件

* GPU
* TPU
* FLOPS (Floating-point operations per second) 每秒峰值速度 或 每秒浮点运算次数
  * MFLOPS   megaFLOPS, 10^6/s, million/s
  * GFLOPS   gigaFLOPS, 10^9/s, billion/s
  * TFLOPS   teraFLOPS, 10^12/s
  * PFLOPS   petaFLOPS, 10^15/s
  * EFLOPS   exaFLOPS, 10^18/s

## GPU

对于训练和测试神经网络而言，GPU比CPU快100倍。GPU中有很多高度并行的核心，非常适合于DL矩阵乘法运算。

几乎所有DL框架都支持*Nvidia* GPU，而很少有支持*AMD* GPU的。

![GPU Performance(FP32, single precision floating point)](https://github.com/AarioAi/share/blob/master/_asset/3-GPU-Performance-FP32.png?raw=true)

上图是32位（FP32）单精度浮点运算峰值（peak performance）性能图

** 使用低精度16位（FP16, half precision）代替32位（single precision）可能更适合神经网络，像双倍精度（FP64, double precision）运算根本没有价值。**

> Storing FP16 (half precision) data compared to higher precision FP32 or FP64 reduces memory usage of the neural network, allowing training and deployment of larger networks, and FP16 data transfers take less time than FP32 or FP64 transfers. Moreover, for many networks deep learning inference can be performed using 8-bit integer (INT8) computations without significant impact on accuracy.
>> In addition to making possible to train and store larger models, switching to FP16 typically gives 2x speed improvement (2x more TFLOPS).

![GPU Performance(FP16, half precision floating point)](https://github.com/AarioAi/share/blob/master/_asset/3-GPU-Performance-FP16.png?raw=true)

### Tensor Cores

 Nvidia Volta 架构新增了 Tensor Cores 可以提升FP16运算能力。2018款GTX游戏显卡（*RTX 2070/2080/2080 Ti*）增加了Tensor Cores。

 **cuDNN 7 开始支持Tensor Cores，支持CNN和RNN正向（forward）、反向（bac）运算。**。由于Tensor Core主要加速FP16（是FP32运算的2倍），而之前（2018年之前）都是FP32。因此若用RTX 2080运行cuDNN，除了去修改程序中浮点数外，还可以通过设置Tensor Core math mode 为 `CUDNN_TENSOR_OP_MATH` （默认为`CUDNN_TENSOR_DEFAULT_METH`），以此禁用Tensor Core运算。

![GPU Performance(Tensor Cores FP16, half precision floating point)](https://github.com/AarioAi/share/blob/master/_asset/3-GPU-Performance-FP16-tensor.png?raw=true)

![GPU Performance Compare)](https://github.com/AarioAi/share/blob/master/_asset/3-GPU-Performance-Compare.png?raw=true)

> **减少显存窍门**：减少 batch size，用FP16或INT8等。

Tensor Core benchmarks性能提升显著：

![Tensor Core Benchmarks](https://github.com/AarioAi/share/blob/master/_asset/3-tensor-core-benchmark.png?raw=true)

### 显存大小和带宽

显存大小（memory size）建议选12G+的。**有些神经网络运算受限于显存带宽，而非运算**，这意味着这种情况下GPU运行效率会远远小于峰值性能（约15～20%的峰值性能）。

## FAQ

## 相关资料

* Nvidia Deep Learning SDK Documentation: Tensor Core Operations
  * https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#tensor_ops
* GPU Compre
  * https://docs.google.com/spreadsheets/d/1xAo6TcSgHdd25EdQ-6GqM0VKbTYu8cWyycgJhHRVIgY/edit#gid=0
* Hardware for Deep Learning. Part 3: GPU
  * https://blog.inten.to/hardware-for-deep-learning-part-3-gpu-8906c1644664
* Yolo Hardware Guide: Neural Networks on GPUs
  * https://pjreddie.com/darknet/hardware-guide/
* **Optimizing Memory Efficiency for Deep Convolutional Neural Networks on GPUs**
  * https://people.engr.ncsu.edu/hzhou/SC-16-DNN.pdf